# Data Preparation Webinar

> **Source**: [data_prep_webinar](https://github.com/qdrant/examples/tree/b3c4b28f66c8cf2a9674bb4491f3f057a1e51237/data_prep_webinar)

This tutorial demonstrates how to prepare and ingest complex, real-world text data into Qdrant, covering essential data cleaning techniques, embedding generation, and advanced filtering capabilities for multilingual datasets.

## Part 1: Core Concept - Why Data Preparation Matters

### The Problem with Raw Data

Most real-world text data comes messy and unstructured. News articles contain HTML tags, special characters, and inconsistent formatting. E-commerce descriptions mix languages. Social media posts include emojis and broken links. Feeding this raw data directly into a vector database produces poor search results because:

- **Noise degrades embeddings**: HTML tags and special characters create irrelevant vector dimensions
- **Inconsistent formatting**: Makes similar content appear different to embedding models
- **Missing metadata**: Lost opportunities for powerful filtering and organization
- **Scale challenges**: Large datasets require efficient batch processing strategies

### The Solution: Systematic Data Preparation

This tutorial shows you how to build a robust data preparation pipeline that transforms messy text into clean, searchable, and filterable vector data. You'll learn to handle multilingual content, create rich metadata payloads, and optimize for search performance.

**What you'll build**: A complete pipeline that ingests news articles, cleans the text, generates embeddings, detects languages, and enables filtered searches like "find English articles about China."

## Part 2: Practical Walkthrough - Building the Pipeline

### Setup and Dependencies

The data preparation pipeline requires several key libraries:

```python
# Core dependencies for the pipeline
!pip install qdrant-client sentence-transformers datasets beautifulsoup4 langdetect

from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, MatchValue
from sentence_transformers import SentenceTransformer
from datasets import load_dataset
import re
from bs4 import BeautifulSoup
from langdetect import detect
```

**Key components:**
- `qdrant-client`: Vector database operations
- `sentence-transformers`: Generate text embeddings
- `datasets`: Load datasets from Hugging Face
- `beautifulsoup4`: Clean HTML content
- `langdetect`: Identify document languages

### Initialize Core Services

```python
# Initialize Qdrant client (local instance)
qdrant_client = QdrantClient("localhost", port=6333)

# Load embedding model
model = SentenceTransformer('all-MiniLM-L6-v2')
```

### Stage 1: Basic Data Ingestion

#### Load and Explore Raw Data

```python
# Load a small news dataset
dataset = load_dataset("Fraser/news-category-dataset", split="train[:1000]")
print(f"Dataset size: {len(dataset)}")
print(f"Columns: {dataset.column_names}")

# Examine a sample record
sample = dataset[0]
print(f"Headline: {sample['headline']}")
print(f"Description: {sample['short_description']}")
```

#### Clean Text Data

Raw text often contains HTML, extra whitespace, and formatting artifacts:

```python
def clean_text(text):
    """Clean text by removing HTML tags and normalizing whitespace"""
    if not text:
        return ""
    
    # Remove HTML tags
    soup = BeautifulSoup(text, 'html.parser')
    cleaned = soup.get_text()
    
    # Normalize whitespace
    cleaned = re.sub(r'\s+', ' ', cleaned)
    cleaned = cleaned.strip()
    
    return cleaned

# Clean the data
cleaned_data = []
for item in dataset:
    cleaned_item = {
        'headline': clean_text(item['headline']),
        'description': clean_text(item['short_description']),
        'category': item['category'],
        'date': item['date']
    }
    cleaned_data.append(cleaned_item)
```

#### Generate Embeddings and Create Collection

```python
# Create Qdrant collection
collection_name = "news_articles"
qdrant_client.recreate_collection(
    collection_name=collection_name,
    vectors_config=VectorParams(size=384, distance=Distance.COSINE)
)

# Process and upload in batches
batch_size = 100
points = []

for i, item in enumerate(cleaned_data):
    # Combine headline and description for embedding
    text_to_embed = f"{item['headline']} {item['description']}"
    
    # Generate embedding
    embedding = model.encode(text_to_embed).tolist()
    
    # Create point with rich payload
    point = PointStruct(
        id=i,
        vector=embedding,
        payload={
            "headline": item['headline'],
            "description": item['description'],
            "category": item['category'],
            "date": item['date'],
            "full_text": text_to_embed
        }
    )
    points.append(point)
    
    # Upload batch when ready
    if len(points) >= batch_size:
        qdrant_client.upsert(collection_name=collection_name, points=points)
        points = []

# Upload remaining points
if points:
    qdrant_client.upsert(collection_name=collection_name, points=points)
```

### Stage 2: Advanced Multilingual Processing

#### Load Multilingual Dataset

```python
# Load a multilingual news dataset
multilingual_dataset = load_dataset("mlsum", "es", split="train[:500]")  # Spanish news

multilingual_data = []
for item in multilingual_dataset:
    cleaned_item = {
        'title': clean_text(item['title']),
        'text': clean_text(item['text'][:500]),  # Truncate for demo
        'topic': item['topic']
    }
    multilingual_data.append(cleaned_item)
```

#### Language Detection and Enhanced Payloads

```python
def detect_language(text):
    """Detect language of text, return 'unknown' if detection fails"""
    try:
        return detect(text)
    except:
        return 'unknown'

# Create new collection for multilingual data
multilingual_collection = "multilingual_news"
qdrant_client.recreate_collection(
    collection_name=multilingual_collection,
    vectors_config=VectorParams(size=384, distance=Distance.COSINE)
)

# Process with language detection
multilingual_points = []
for i, item in enumerate(multilingual_data):
    text_content = f"{item['title']} {item['text']}"
    
    # Detect language
    language = detect_language(text_content)
    
    # Generate embedding
    embedding = model.encode(text_content).tolist()
    
    # Create enhanced point
    point = PointStruct(
        id=i,
        vector=embedding,
        payload={
            "title": item['title'],
            "text": item['text'],
            "topic": item['topic'],
            "language": language,
            "full_content": text_content
        }
    )
    multilingual_points.append(point)

# Upload multilingual data
qdrant_client.upsert(collection_name=multilingual_collection, points=multilingual_points)
```

#### Create Payload Index for Efficient Filtering

```python
# Create index on language field for fast filtering
qdrant_client.create_payload_index(
    collection_name=multilingual_collection,
    field_name="language",
    field_schema="keyword"
)

# Also index topic for additional filtering capabilities
qdrant_client.create_payload_index(
    collection_name=multilingual_collection,
    field_name="topic",
    field_schema="keyword"
)
```

### Stage 3: Advanced Search with Filtering

#### Perform Filtered Vector Search

```python
# Search for content about "politics" in Spanish only
query_text = "pol√≠tica gobierno elecciones"
query_embedding = model.encode(query_text).tolist()

# Create language filter
language_filter = Filter(
    must=[
        FieldCondition(
            key="language",
            match=MatchValue(value="es")
        )
    ]
)

# Perform filtered search
search_results = qdrant_client.search(
    collection_name=multilingual_collection,
    query_vector=query_embedding,
    query_filter=language_filter,
    limit=5
)

# Display results
for result in search_results:
    print(f"Score: {result.score:.3f}")
    print(f"Title: {result.payload['title']}")
    print(f"Language: {result.payload['language']}")
    print(f"Topic: {result.payload['topic']}")
    print("---")
```

#### Multi-Criteria Filtering

```python
# Complex filter: Spanish politics articles
complex_filter = Filter(
    must=[
        FieldCondition(key="language", match=MatchValue(value="es")),
        FieldCondition(key="topic", match=MatchValue(value="politics"))
    ]
)

complex_results = qdrant_client.search(
    collection_name=multilingual_collection,
    query_vector=query_embedding,
    query_filter=complex_filter,
    limit=3
)
```

## Part 3: Mental Models & Deep Dives

### Understanding the Data Preparation Pipeline

Think of data preparation as creating a well-organized library from a pile of random books and papers:

**Raw Data (The Pile)**
- Books with torn pages (HTML tags)
- Papers in different languages
- Inconsistent labeling
- Random organization

**Cleaned Data (The Organized Library)**
- Clean, readable text
- Consistent formatting
- Proper categorization
- Efficient search system

### The Embedding Quality Principle

**Mental Model**: Embeddings are like fingerprints for text meaning. Clean input creates distinct, meaningful fingerprints; dirty input creates blurry, confusing ones.

**Why cleaning matters**:
- HTML tags like `<div>` create noise in the vector space
- Inconsistent whitespace makes similar content appear different
- Special characters can dominate embedding dimensions

### The Metadata Strategy

**Mental Model**: Think of payloads as index cards in a library system. The vector finds books with similar content; the metadata helps you filter by author, year, genre, etc.

**Strategic payload design**:
```python
# Good payload structure
payload = {
    "content": clean_text,           # Searchable content
    "language": detected_language,   # Filterable attribute
    "category": article_category,    # Hierarchical filtering
    "date": publication_date,        # Temporal filtering
    "source": data_source           # Provenance tracking
}
```

### Batch Processing Mental Model

**Think of batch processing like loading a truck**:
- Don't make 1000 trips with one item each
- Fill the truck (batch) before driving (uploading)
- Balance truck capacity (memory) with trip efficiency

**Optimal batch sizing**:
```python
# Consider these factors for batch size
- Available RAM (larger batches need more memory)
- Network latency (fewer requests preferred)
- Error recovery (smaller batches easier to retry)
- Progress tracking (frequent updates vs efficiency)
```

### Language Detection and Indexing Strategy

**Mental Model**: Language detection + indexing is like creating specialized sections in a multilingual library with fast lookup catalogs.

**Why index metadata fields**:
- **Speed**: O(1) lookup vs O(n) scanning
- **Precision**: Exact matches on categorical data
- **Scalability**: Performance doesn't degrade with collection size

### Common Pitfalls and Solutions

#### Pitfall 1: Poor Text Cleaning
```python
# Bad: Minimal cleaning
text = raw_html_content.strip()

# Good: Comprehensive cleaning
def robust_clean(text):
    soup = BeautifulSoup(text, 'html.parser')
    cleaned = soup.get_text()
    cleaned = re.sub(r'\s+', ' ', cleaned)
    cleaned = re.sub(r'[^\w\s]', ' ', cleaned)  # Remove special chars
    return cleaned.strip().lower()
```

#### Pitfall 2: Ignoring Batch Size Optimization
```python
# Bad: One-by-one insertion
for item in large_dataset:
    qdrant_client.upsert(collection_name, [point])

# Good: Batched insertion
batch = []
for item in large_dataset:
    batch.append(point)
    if len(batch) >= optimal_batch_size:
        qdrant_client.upsert(collection_name, batch)
        batch = []
```

#### Pitfall 3: Missing Payload Indexes
```python
# Create indexes for frequently filtered fields
for field in ["language", "category", "source"]:
    qdrant_client.create_payload_index(
        collection_name=collection_name,
        field_name=field,
        field_schema="keyword"
    )
```

### Real-World Applications

**E-commerce Product Search**
- Clean product descriptions, detect languages
- Filter by category, brand, price range
- Enable "find similar products in English under $50"

**Document Management Systems**
- Process PDFs, Word docs, emails
- Index by department, date, document type
- Search "technical docs from engineering team this quarter"

**Content Recommendation**
- Clean user-generated content
- Detect language, extract topics
- Recommend "similar articles in user's preferred language"

### Performance Optimization Strategies

**Embedding Model Selection**
- Balance model size vs quality vs speed
- Consider domain-specific models
- Benchmark on your actual data

**Collection Configuration**
```python
# Optimize for your use case
VectorParams(
    size=384,                    # Smaller = faster, larger = more precise
    distance=Distance.COSINE,    # Good for text similarity
    on_disk=True                # Save RAM for large collections
)
```

**Memory Management**
```python
# Process large datasets efficiently
def process_large_dataset(dataset, batch_size=1000):
    for i in range(0, len(dataset), batch_size):
        batch = dataset[i:i+batch_size]
        process_batch(batch)
        # Clear memory between batches
        gc.collect()
```

This comprehensive approach to data preparation ensures your Qdrant vector database delivers fast, accurate, and filterable search results across diverse, multilingual content.